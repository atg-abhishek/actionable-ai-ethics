

<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Fairness Definitions Explained &#8212; Actionable AI Ethics</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-dropdown.css" />
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/sphinx-book-theme.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://atg-abhishek.github.io/actionable-ai-ethics/fairness_definitions_explained.html" />
    <link rel="shortcut icon" href="_static/abhishek.png"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Disability, Bias, and AI" href="disability_bias_ai.html" />
    <link rel="prev" title="Bias and Fairness" href="bias_and_fairness.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">


<!-- Opengraph tags -->
<meta property="og:url"         content="https://atg-abhishek.github.io/actionable-ai-ethics/fairness_definitions_explained.html" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="Fairness Definitions Explained" />
<meta property="og:description" content="Fairness Definitions Explained  Authors: Sahil Verma and Julia Rubin  Paper link  Summary  One-line summary  The basic premise of the paper is that it will atte" />
<meta property="og:image"       content="https://atg-abhishek.github.io/actionable-ai-ethics/_static/abhishek.png" />

<meta name="twitter:card" content="summary" />


  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="index.html">
  
  <img src="_static/abhishek.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Actionable AI Ethics</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Welcome to Actionable AI Ethics
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Papers
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="papers.html">
   Papers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="privacy.html">
   Privacy
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="reference internal" href="bias_and_fairness.html">
   Bias and Fairness
  </a>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Fairness Definitions Explained
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="disability_bias_ai.html">
     Disability, Bias, and AI
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="understanding_cap_limits_societal_implications_large_language_models.html">
     Understanding the Capabilities, Limitations, and Societal Impacts of Large Language Models
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="traceability_and_auditability.html">
   Traceability and Auditability
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="interpretability_and_explainability.html">
   Interpretability and Explainability
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="machine_learning_security.html">
   Machine Learning Security
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="others.html">
   Others
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Tools
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="tools.html">
   AI Ethics Tool of the Week
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Books
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="books.html">
   Books
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  You can support this work by <a href="https://buymeacoffee.com/abhishekgupta">buying me a coffee!</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/fairness_definitions_explained.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary">
   Summary
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#one-line-summary">
     One-line summary
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data-and-techniques-used">
     Data and techniques used
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#definitions">
     Definitions
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#statistical-definitions">
       Statistical Definitions
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#basics">
         Basics
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#intermediate">
         Intermediate
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#definitions-based-on-predicted-outcomes">
       Definitions based on predicted outcomes
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#definitions-based-on-predicted-and-actual-outcomes">
       Definitions based on predicted and actual outcomes
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#definitions-based-on-predicted-probabilities-and-actual-outcomes">
       Definitions based on predicted probabilities and actual outcomes
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#similarity-based-measures">
       Similarity based measures
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#causal-reasoning">
       Causal Reasoning
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#conclusion">
     Conclusion
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-does-this-mean-for-actionable-ai-ethics">
   What does this mean for Actionable AI Ethics?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#questions-i-am-exploring">
   Questions I am exploring
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#potential-further-reading">
   Potential further reading
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#twitter-discussion">
   Twitter discussion
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sign-up-for-the-newsletter">
     Sign up for the newsletter
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#support-me-with-a-coffee">
     Support me with a coffee
    </a>
   </li>
  </ul>
 </li>
</ul>

        </nav>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="fairness-definitions-explained">
<h1>Fairness Definitions Explained<a class="headerlink" href="#fairness-definitions-explained" title="Permalink to this headline">¶</a></h1>
<p><strong>Authors: Sahil Verma and Julia Rubin</strong></p>
<p><a class="reference external" href="https://dl.acm.org/citation.cfm?id=3194776">Paper link</a></p>
<div class="section" id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h2>
<div class="section" id="one-line-summary">
<h3>One-line summary<a class="headerlink" href="#one-line-summary" title="Permalink to this headline">¶</a></h3>
<p>The basic premise of the paper is that it will attempt to explain how because of different definitions of fairness, we can have scenarios that are fair according to one and not fair according to another.</p>
<iframe src="https://actionableaiethics.substack.com/embed" width="480" height="320" style="border:1px solid #EEE; background:white;" frameborder="0" scrolling="no"></iframe>
<p>If you enjoy the content on this page, you can support my work by <a class="reference external" href="https://buymeacoffee.com/abhishekgupta">buying me a coffee</a></p>
<script type="text/javascript" src="https://cdnjs.buymeacoffee.com/1.0.0/button.prod.min.js" data-name="bmc-button" data-slug="abhishekgupta" data-color="#FF5F5F" data-emoji=""  data-font="Cookie" data-text="Buy me a coffee" data-outline-color="#000000" data-font-color="#ffffff" data-coffee-color="#FFDD00" ></script>
</div>
<div class="section" id="data-and-techniques-used">
<h3>Data and techniques used<a class="headerlink" href="#data-and-techniques-used" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>As an example it uses the <a class="reference external" href="https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)">German Credit Dataset</a> and checks if there are any biases based on gender. Given prior work that has shown how some definitions are necessarily incompatible with each other, the findings from this paper bear the same in terms of the final results.</p></li>
<li><p>Given that there was so case of a single female applicant, the authors don’t take that into consideration and in making cross-gender comparisons, they just look at married and divorced males and females.</p></li>
<li><p>They used a logistic regression model with 10-fold cross-validation using 90% of the dataset as training data and 10% as test data. The numerical and categorical variables were used directly and the categorical variables were converted to binary attributed giving a total of 48 attributes.</p></li>
</ul>
</div>
<div class="section" id="definitions">
<h3>Definitions<a class="headerlink" href="#definitions" title="Permalink to this headline">¶</a></h3>
<p><em>There are many, many definitions in this paper, and this is my attempt at making the most sense of them and keeping them all organized for easy reference for the rest of the summary</em></p>
<p>I’ll try and steer away from using mathematical notation because there will be a lot of symbols and I feel that it will be easier to get a sense for each of these definitions when expressed in natural language.</p>
<div class="section" id="statistical-definitions">
<h4>Statistical Definitions<a class="headerlink" href="#statistical-definitions" title="Permalink to this headline">¶</a></h4>
<div class="section" id="basics">
<h5>Basics<a class="headerlink" href="#basics" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li><p><strong>True Positive (TP)</strong> : when the predicted outcome and the actual outcome are both in the positive class</p></li>
<li><p><strong>True Negative (TN)</strong> : when the predicted outcome and the actual outcome are both in the negative class</p></li>
</ul>
<p><em>So, both of the above are the cases when you are spot on!</em></p>
<ul class="simple">
<li><p><strong>False Positive (FP)</strong> : when the predicted outcome is positive <strong>but</strong> the actual outcome is negative.</p></li>
<li><p><strong>False Negative (FN)</strong> : when the predicted outcome is negative <strong>but</strong> the actual outcome is positive.</p></li>
</ul>
<p><em>These are both cases where a mismatch has occurred.</em></p>
<p><strong>My recommendation for these first 4 definitions if you’re encountering these for the first time is to stick to keeping in mind one from each bucket (and remembering that well), the other one in that bucket is just the opposite of that.</strong></p>
<p><em>Make sure that you really get these before we move on to the other definitions since they will use these as building blocks!</em></p>
</div>
<div class="section" id="intermediate">
<h5>Intermediate<a class="headerlink" href="#intermediate" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li><p><strong>Positive Predictive Value (PPV)</strong> : Divide the correct positive predictions that you made by <em>all</em> the cases where you predicted positive outcomes. <strong>This is also called precision</strong> (a term that you will encounter frequently in the field)</p></li>
<li><p><strong>Negative Predictive Value (NPV)</strong> : Similar to the one above, divide the correct negative predictions that you made by <em>all</em> the cases where you predicted negative outcomes. <em>You can choose to just remember the previous definition and you’ll know that this is just the case for the negative classes.</em></p></li>
<li><p><strong>False Discovery Rate (FDR)</strong> : This is the probability of false (or wrong) acceptance.</p></li>
<li><p><strong>False Omission Rate (FOR)</strong> : This is the probability of wrongly rejecting someone. <em>Again, here you can choose to just remember the previous one and know that this is the opposite case.</em></p></li>
</ul>
<p><em>So, these were all definitions where we were only looking at the predicted outcomes, the difference in the following definitions is that we will also look at the actual outcomes.</em></p>
<ul class="simple">
<li><p><strong>True Positive Rate (TPR)</strong> : This is the probability of identifying a truly positive subject as such. <strong>This is also called as sensitivity or recall.</strong> (common references for this term in the literature)</p></li>
<li><p><strong>True Negative Rate (TNR)</strong> : This is the probability of identifying a truly negative subject as such. <em>Again, here you can choose to just remember the previous one and know that this is the opposite case.</em></p></li>
</ul>
<p><em>The above two definitions are the cases of correct identification amongst all the actual cases belonging to that class.</em></p>
<ul class="simple">
<li><p><strong>False Positive Rate (FPR)</strong> : This is the probability of false alarms, that is falsely accepting a negative case.</p></li>
<li><p><strong>False Negative Rate (FNR)</strong> : This is the probability of getting a negative result for actually positive cases.</p></li>
</ul>
</div>
</div>
<div class="section" id="definitions-based-on-predicted-outcomes">
<h4>Definitions based on predicted outcomes<a class="headerlink" href="#definitions-based-on-predicted-outcomes" title="Permalink to this headline">¶</a></h4>
<p>Let’s give a quick definition for <strong>protected attributes</strong> : these are attributes which are <em>sensitive</em>, which are the subject of potential discrimination. Someone belong to the protected group then is the one who has a positive value for this attribute and the unprotected group is the one that has negative values for this attribute.</p>
<ul class="simple">
<li><p><strong>Group Fairness</strong> : Members from both the protected and unprotected group have the same probability of being assigned to the positive predicted class. In the case of the credit example, this means that regardless of gender, you have an equal probability of being assigned a good credit score.</p></li>
</ul>
<p><em>Note that the above is also called equal acceptance rate, benchmarking, and statistical parity (not to be confused with the following definition)</em></p>
<ul class="simple">
<li><p><strong>Conditional Statistical Parity</strong> : Extending the previous definitions, this definition allows for the inclusion of a group of legitimate factors that won’t be considered discriminatory when included in the decision making process. So, controlling for these factors, the groups should have equal probabilities of landing with positive predicted values.</p></li>
</ul>
</div>
<div class="section" id="definitions-based-on-predicted-and-actual-outcomes">
<h4>Definitions based on predicted and actual outcomes<a class="headerlink" href="#definitions-based-on-predicted-and-actual-outcomes" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p><strong>Predictive Parity</strong> : The fraction of correctly predicted positive cases should be the same across the protected and the unprotected group.</p></li>
</ul>
<p><em>This is also called outcome test</em></p>
<ul class="simple">
<li><p><strong>False positive error rate balance</strong> : Getting similar results for both groups when they actually belong to the negative class.</p></li>
</ul>
<p><em>This is also called predictive equality</em></p>
<ul class="simple">
<li><p><strong>False negative error rate balance</strong> : The probability of a subject in a positive class to have a negative predictive value should be the same across the groups. A thing to note here is that</p></li>
</ul>
<p><em>This is also called equal opportunity</em></p>
<ul class="simple">
<li><p><strong>Equalized odds</strong> : Combining the previous two definitions, this definition tells us that, across the two groups, we should get a similar classification for those with actual positive outcomes and actual negative outcomes.</p></li>
</ul>
<p><em>This is also called disparate mistreatment and conditional procedure accuracy equality</em></p>
<ul class="simple">
<li><p><strong>Conditional use accuracy equality</strong> : This definition gives equivalent accuracy, across both groups, for both positive and negative predicted classes.</p></li>
<li><p><strong>Overall accuracy equality</strong> : Across both groups, this definition gives equal prediction accuracy for those who are in the positive and negative classes to be correctly assigned to those classes. <strong>A thing to note here is that we are assuming that we value equally true positives and true negatives which might not always be the case.</strong></p></li>
<li><p><strong>Treatment equality</strong> : Across both groups, this definition looks that the ratio of errors made is the same (i.e. the ratio of false positives to false negatives).</p></li>
</ul>
</div>
<div class="section" id="definitions-based-on-predicted-probabilities-and-actual-outcomes">
<h4>Definitions based on predicted probabilities and actual outcomes<a class="headerlink" href="#definitions-based-on-predicted-probabilities-and-actual-outcomes" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p><strong>Test fairness</strong> : Given the probability score from the classifier, across both groups, this definition checks if the subjects have equal probability of truly belonging to the positive class.</p></li>
</ul>
<p><em>This is also called matching conditional frequencies and callibration</em></p>
<ul class="simple">
<li><p><strong>Well-callibration</strong> : This extends the previous definition by saying that not only should the probability of truly belonging to the positive class is the same across the groups but that it is also equal to the predicted probability score. The reason to do so is that we are trying to <em>callibrate</em> the classifier by checking if it indeed meets the base rate prevalence for truly belonging to the positive class as it predicts.</p></li>
<li><p><strong>Balance for positive class</strong> : The subjects constituting the positive class from both groups have equal average predicted probability scores.</p></li>
<li><p><strong>Balance for negative class</strong> : This is the flipped version of the previous definition.</p></li>
</ul>
</div>
<div class="section" id="similarity-based-measures">
<h4>Similarity based measures<a class="headerlink" href="#similarity-based-measures" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p><strong>Causal discrimination</strong> : Satisfying this definition means that if all the attributes except the <em>protected attribute</em> are the same, then we should get the same predicted outcomes across both groups.</p></li>
<li><p><strong>Fairness through unawareness</strong> : This definition is essentially the same as the previous one, <strong>except</strong> that we don’t include the <em>protected attributes</em> in making the decisions.</p></li>
<li><p><strong>Fairness through awareness</strong> : Expanding the previous two definitions, similar subjects should have similar classification.</p></li>
</ul>
</div>
<div class="section" id="causal-reasoning">
<h4>Causal Reasoning<a class="headerlink" href="#causal-reasoning" title="Permalink to this headline">¶</a></h4>
<p><strong>What are causal graphs?</strong></p>
<p>They are basically a way to map causal (cause-effect) reasoning and expressing them as nodes and edges helps to understand how one thing influences another.</p>
<p><strong>What is a proxy attribute?</strong></p>
<p>Alright, so this is a big one, we often talk about this in the discussions about fairness and bias.
In the context of a causal graph, it is an attribute that can be used to derive the values in another attribute. <em>C’est tout!</em> (Translation: That’s all!)</p>
<ul class="simple">
<li><p><strong>Counterfactual fairness</strong> : If the predicted outcome in the graph is not dependent on any of the descendants of the protected attribute.</p></li>
</ul>
<p><strong>What is a resolving attribute?</strong></p>
<p>An attribute that is influenced by the protected attribute but in a non-discriminatory way.</p>
<ul class="simple">
<li><p><strong>No unresolved discrimination</strong> : If the only paths from the protected attributes to the predicted outcome in the causal graph are through a resolving attribute, then we satisfy this definition.</p></li>
<li><p><strong>No proxy discrimination</strong> : We satisfy this definition if there is no path from the protected attribute to the predicted outcome that is blocked by a proxy attribute.</p></li>
<li><p><strong>Fair inference</strong> : We classify the different paths from the protected attribute to the predicted outcome as legitimiate or illegitimate. Some attributes even if they are proxies might be considered legitimate in making a prediction and hence the path could be classified as legitimate. We satisfy this definition then if there are no illegitimate paths in the causal graph.</p></li>
</ul>
<p><em>For the above 3 definitions, I strongly recommend looking at the graphic in the paper and then working through the definition description in the paper as they are slightly complicated and require a couple of pass-throughs to make sense of them.</em></p>
</div>
</div>
<div class="section" id="conclusion">
<h3>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>One of the things to keep in mind about the statistical definition is that even though they are easy to measure, we need to have verified, measured outcomes, if those are not available there is a bit of a problem. That is, even though we have a certain distribution for the training data, there is <strong>no guarantee</strong> that it holds for the real-world data on which the system will operate.</p></li>
<li><p>The causal definitions and similarity based metrics require expert input and are hence subject to expert bias.</p></li>
<li><p>For some definitions like <em>fairness through unawareness</em> it requires that we are able to find individuals who differ in some attributes but are identical in others. This is not always (in fact quite rarely!) possible in the real-world and hence limit the viability of these approaches.</p></li>
</ul>
</div>
</div>
<div class="section" id="what-does-this-mean-for-actionable-ai-ethics">
<h2>What does this mean for Actionable AI Ethics?<a class="headerlink" href="#what-does-this-mean-for-actionable-ai-ethics" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>I believe that just having a basic awareness of the various definitions of fairness is a great starting point.</p></li>
<li><p>From a practical standpoint, it is important to know when a certain product or service is deemed to be fair and what can be done in the case that are competing definitions that might be applicable.</p></li>
<li><p>My takeaway from this paper is that the involvement of domain experts is crucial in selecting the right definition of fairness.</p></li>
<li><p>It is also important to be transparent with the users of the system on which definitions of fairness you are utilizing and what the shortcomings of that are.</p></li>
</ul>
</div>
<div class="section" id="questions-i-am-exploring">
<h2>Questions I am exploring<a class="headerlink" href="#questions-i-am-exploring" title="Permalink to this headline">¶</a></h2>
<p><strong>If you have answers to any of these questions, please <a class="reference external" href="https://twitter.com/actionable_ai/status/1321332124490911744?s=20">tweet</a> and let me know!</strong></p>
<ol class="simple">
<li><p>Why is it that there are no single females in the dataset?</p></li>
<li><p>What are some other datasets that are similar to this that can be used for comparing fairness Definitions?</p></li>
<li><p>Is the binary good or bad credit decision sufficient or should there be more graduations for involving a human in the loop?</p></li>
<li><p>What are some classes of basic definitions that the policy makers are familiar with? Does the plethora of definitions make it hard for them to make comparisons?</p></li>
<li><p>What would be required to get to a more standardized taxonomy?</p></li>
<li><p>Is there a simple way to communicate the confusion matrix to non-technical audiences?</p></li>
<li><p>What constitutes a minor difference to assign a fair outcome? Example of the difference between the male and female case is 0.03 in conditional statistical parity and that’s assigned as the same for practical purposes</p></li>
<li><p>Given the differences across various definitions, should we require the computation of all for the classification work so that people can make decisions for themselves?</p></li>
<li><p>What about intersectionality and people who are non-binary or genderfluid? Since that data isn’t captured here, there might be over- or underestimation in the results.</p></li>
</ol>
</div>
<div class="section" id="potential-further-reading">
<h2>Potential further reading<a class="headerlink" href="#potential-further-reading" title="Permalink to this headline">¶</a></h2>
<p>A list of papers that I think might be interesting related to this paper.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/1808.00023">The Measure and Mismeasure of Fairness: A Critical Review of Fair Machine Learning</a></p></li>
<li><p><a class="reference external" href="https://dl.acm.org/doi/abs/10.1145/3287560.3287573">Measuring the Biases that Matter: The Ethical and Casual Foundations for Measures of Fairness in Algorithms</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2001.07864">Fairness Metrics: A Comparative Analysis</a></p></li>
</ul>
<p><em>Please note that this is a wish list of sorts and I haven’t read through the papers listed here unless specified otherwise (if I have read them, there will be a link from the entry to the page for that.)</em></p>
</div>
<div class="section" id="twitter-discussion">
<h2>Twitter discussion<a class="headerlink" href="#twitter-discussion" title="Permalink to this headline">¶</a></h2>
<p>I’ll write back here with interesting points that surface from the Twitter discussion.</p>
<p><em>If you have comments and would like to discuss more, please leave a <a class="reference external" href="https://twitter.com/actionable_ai/status/1321332124490911744?s=20">tweet here</a>.</em></p>
<div class="section" id="sign-up-for-the-newsletter">
<h3>Sign up for the newsletter<a class="headerlink" href="#sign-up-for-the-newsletter" title="Permalink to this headline">¶</a></h3>
<iframe src="https://actionableaiethics.substack.com/embed" width="480" height="320" style="border:1px solid #EEE; background:white;" frameborder="0" scrolling="no"></iframe>
<p>To stay up-to-date with the latest content in terms of what I am reading and thinking about, please subscribe to the <a class="reference external" href="https://actionableaiethics.substack.com">Actionable AI Ethics newsletter</a></p>
</div>
<div class="section" id="support-me-with-a-coffee">
<h3>Support me with a coffee<a class="headerlink" href="#support-me-with-a-coffee" title="Permalink to this headline">¶</a></h3>
<p>If you enjoy the content on this page, you can support my work by <a class="reference external" href="https://buymeacoffee.com/abhishekgupta">buying me a coffee</a></p>
<script type="text/javascript" src="https://cdnjs.buymeacoffee.com/1.0.0/button.prod.min.js" data-name="bmc-button" data-slug="abhishekgupta" data-color="#FF5F5F" data-emoji=""  data-font="Cookie" data-text="Buy me a coffee" data-outline-color="#000000" data-font-color="#ffffff" data-coffee-color="#FFDD00" ></script></div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="bias_and_fairness.html" title="previous page">Bias and Fairness</a>
    <a class='right-next' id="next-link" href="disability_bias_ai.html" title="next page">Disability, Bias, and AI</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Abhishek Gupta<br/>
        
            &copy; Copyright 2021.<br/>
          <div class="extra_footer">
            Making Responsible AI the Norm rather than the Exception
          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="_static/js/index.js"></script>
    
  </body>
</html>