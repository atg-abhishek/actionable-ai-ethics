

<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Hazard Contribution Modes of Machine Learning Components &#8212; Actionable AI Ethics</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Project Let’s Talk Privacy Full Report" href="lets_talk_privacy.html" />
    <link rel="prev" title="Automating informality: On AI and labour in the global South" href="automating_informality.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="index.html">
  
  <img src="_static/abhishek.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Actionable AI Ethics</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Welcome to Actionable AI Ethics
  </a>
 </li>
</ul>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 current active">
  <a class="reference internal" href="readings.html">
   Readings
  </a>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="reference internal" href="papers.html">
     Papers
    </a>
    <ul class="current">
     <li class="toctree-l3">
      <a class="reference internal" href="fairness_definitions_explained.html">
       Fairness Definitions Explained
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="automating_informality.html">
       Automating informality: On AI and labour in the global South
      </a>
     </li>
     <li class="toctree-l3 current active">
      <a class="current reference internal" href="#">
       Hazard Contribution Modes of Machine Learning Components
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="lets_talk_privacy.html">
       Project Let’s Talk Privacy Full Report
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="books.html">
     Books
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="tools.html">
   AI Ethics Tool of the Week
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/hazard_modes.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary">
   Summary
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#one-line-summary">
     One-line summary
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#definitions">
     Definitions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#why-is-this-a-problem-in-lecs">
     Why is this a problem in LECs?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#how-are-hcms-different-from-failure-modes">
     How are HCMs different from failure modes?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#how-does-lec-error-arise">
     How does LEC error arise?
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#supervised-learning">
       Supervised learning
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#unsupervised-learning">
       Unsupervised learning
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-is-expected-performance">
     What is expected performance?
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#expected-and-accurate-ea-hcm">
       Expected and Accurate (EA) HCM
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#expected-with-error-ee-hcm">
       Expected with Error (EE) HCM
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-is-unexpected-performance">
     What is unexpected performance?
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#how-do-we-avoid-this-unexpected-performance">
       How do we avoid this unexpected performance?
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#unexpected-and-accurate-ua-hcm">
       Unexpected and accurate (UA) HCM
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#so-how-do-we-manage-hcms">
     So how do we manage HCMs?
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#under-accurate-and-hazardous-output">
       Under accurate and hazardous output
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#under-hazardous-output-with-error">
       Under hazardous output with error
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#potential-approaches">
       Potential approaches
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#safety-assurance-arguments">
     Safety Assurance Arguments
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#conclusion">
     Conclusion
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-does-this-mean-for-actionable-ai-ethics">
     What does this mean for Actionable AI Ethics?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#questions-that-i-am-exploring">
     Questions that I am exploring
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#potential-further-reading">
     Potential further reading
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#twitter-discussion">
     Twitter discussion
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sign-up-for-the-newsletter">
     Sign up for the newsletter
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#support-me-with-a-coffee">
     Support me with a coffee
    </a>
   </li>
  </ul>
 </li>
</ul>

        </nav>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="hazard-contribution-modes-of-machine-learning-components">
<h1>Hazard Contribution Modes of Machine Learning Components<a class="headerlink" href="#hazard-contribution-modes-of-machine-learning-components" title="Permalink to this headline">¶</a></h1>
<p><strong>Authors</strong>: Colin Smith, Ewen Denney, and Ganesh Pai</p>
<p><a class="reference external" href="https://ntrs.nasa.gov/citations/20200001851">Paper link</a></p>
<div class="section" id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h2>
<div class="section" id="one-line-summary">
<h3>One-line summary<a class="headerlink" href="#one-line-summary" title="Permalink to this headline">¶</a></h3>
<p>This paper provides a categorization framework for assessing the safety posture of a system that consists of embedded machine learning components. It additionally ties that in with a safety assurance reasoning scheme that helps to provide justifiable and demonstrable mechanisms for proving the safety of the system.</p>
<iframe src="https://actionableaiethics.substack.com/embed" width="480" height="320" style="border:1px solid #EEE; background:white;" frameborder="0" scrolling="no"></iframe>
<p>If you enjoy the content on this page, you can support my work by <a class="reference external" href="https://buymeacoffee.com/abhishekgupta">buying me a coffee</a></p>
<script type="text/javascript" src="https://cdnjs.buymeacoffee.com/1.0.0/button.prod.min.js" data-name="bmc-button" data-slug="abhishekgupta" data-color="#FF5F5F" data-emoji=""  data-font="Cookie" data-text="Buy me a coffee" data-outline-color="#000000" data-font-color="#ffffff" data-coffee-color="#FFDD00" ></script>
</div>
<div class="section" id="definitions">
<h3>Definitions<a class="headerlink" href="#definitions" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p><strong>Safety assurance arguments</strong> : Structured reasoning that links the safety claims made about a system with verifiable and auditable pieces of evidence. Specifically, it provides defensible and comprehensive justification for the safety of a system in a given environment.</p></li>
<li><p><strong>Failure modes and effects analysis (FMEA)</strong> : A bottom-up hazard analysis approach for a system with a focus on the level of components of the system.</p></li>
<li><p><strong>Learning-enabled components (LEC)</strong> : Software systems that consist of knowledge acquisition and machine learning components to provide a function or a service.</p></li>
<li><p><strong>Hazard Contribution Mode (HCM)</strong> : A categorization of the ways in which LEC outputs can lead to hazardous system states. It does so by linking the potentially unobservable outputs of the LEC with the observable outputs of the wider system.</p></li>
<li><p><strong>LEC accuracy</strong> : The property of the LEC such that its outputs are globally optimal which could mean accurate classification, near-perfect regression, or selection of reward-maximization actions in reinforcement learning.</p></li>
<li><p><strong>LEC error</strong> : The divergence between the LEC outputs and the reference values.</p></li>
</ul>
</div>
<div class="section" id="why-is-this-a-problem-in-lecs">
<h3>Why is this a problem in LECs?<a class="headerlink" href="#why-is-this-a-problem-in-lecs" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>The problems where machine learning is utilized is often hard to specify, if it was easy to specify, it might not have warranted the use of machine learning.</p></li>
<li><p>ML algorithms are often unintelligible to humans which makes their oversight hard.</p></li>
</ul>
</div>
<div class="section" id="how-are-hcms-different-from-failure-modes">
<h3>How are HCMs different from failure modes?<a class="headerlink" href="#how-are-hcms-different-from-failure-modes" title="Permalink to this headline">¶</a></h3>
<p>They are broader than just failure modes in the sense that they also encapsulate the cases where there isn’t a failure but it can still lead to a hazardous state.</p>
<p><strong>Figure 2 provided in the paper is a really good reference for someone who wants to get a quick understanding of the spectrum of HCMs</strong></p>
<p><img alt="HCM" src="_images/hcm.png" /></p>
<p>One of the things that is important regarding this figure is that we assume that we have the ability to define the safety thresholds and states for the system as a whole.</p>
<p>Keeping this in mind, violating the safety thresholds can thus lead to the transition of the system from a safe to a hazardous state.</p>
</div>
<div class="section" id="how-does-lec-error-arise">
<h3>How does LEC error arise?<a class="headerlink" href="#how-does-lec-error-arise" title="Permalink to this headline">¶</a></h3>
<div class="section" id="supervised-learning">
<h4>Supervised learning<a class="headerlink" href="#supervised-learning" title="Permalink to this headline">¶</a></h4>
<p>Error can emerge from a combination of <em>bias</em> from the model assumptions and from <em>variance</em> from the type and quantity of data used and <em>noise</em>.</p>
</div>
<div class="section" id="unsupervised-learning">
<h4>Unsupervised learning<a class="headerlink" href="#unsupervised-learning" title="Permalink to this headline">¶</a></h4>
<p>There are several sources for errors in this context: inadequately specified optimization objective leading to <em>approximation errors</em>, an <em>identifiability error</em> from the model parameter choices that complicate the differentiation of different inputs, inadequate data a.k.a. <em>estimation error</em>, and algorithmic insufficiencies a.k.a. <em>optimization errors</em>.</p>
</div>
</div>
<div class="section" id="what-is-expected-performance">
<h3>What is expected performance?<a class="headerlink" href="#what-is-expected-performance" title="Permalink to this headline">¶</a></h3>
<p>One of the important notions that is utilized in this paper is <em>expected performance</em>: this is basically the idea that an LEC performs as expected, be that giving an accurate output or an erroneous one. What it essentially means is that you will see outputs from the system, whether accurate or erroneous, as was exhibited during the training, validation, and calibration phases of the LEC.</p>
<div class="section" id="expected-and-accurate-ea-hcm">
<h4>Expected and Accurate (EA) HCM<a class="headerlink" href="#expected-and-accurate-ea-hcm" title="Permalink to this headline">¶</a></h4>
<p>The system gives outputs that are accurate (as per the specification of the system) yet they induce hazardous system states.</p>
<p><strong>An example</strong> : Say you have an autonomous vehicle (AV) that relies on lane markings to localize itself, in this case, if some of those lane markings are removed for some reason, the sensors might <em>latch on</em> to other lane markings and thus correctly align themselves with that and maintain appropriate distance except that they are <strong>not</strong> in the right lane and can lead to an accident.</p>
<p><strong>When can this arise?</strong> : This can happen in the case of sensor malfunctions or <em>different-from-normal</em> environmental conditions within which the system is operating. This might happen because of inadequate representation in the data that is used to train the system or in the optimization algorithm that is used.</p>
</div>
<div class="section" id="expected-with-error-ee-hcm">
<h4>Expected with Error (EE) HCM<a class="headerlink" href="#expected-with-error-ee-hcm" title="Permalink to this headline">¶</a></h4>
<p>The system gives outputs that are erroneous with such a large magnitude of error that they violate the safety thresholds of the system.</p>
<p><strong>An example</strong> : Building on the previous example, if the lane-keeping function has some deviation in the outputs whereby the errors are so large that, for example in estimating the distance to keep in the lane, instead of keeping the lane, it leads to lane departure and hence a hazardous state.</p>
<p>*A special note here that adversarial perturbations *</p>
<p><strong>When can this arise?</strong> : In the cases where there is inadequate representation in the training data, assumptions made in the building on the ML model, especially as it relates to the relationship required between the inputs, outputs, and the safety thresholds, and an inadequate coverage of the environmental conditions within which the system will be deployed.</p>
</div>
</div>
<div class="section" id="what-is-unexpected-performance">
<h3>What is unexpected performance?<a class="headerlink" href="#what-is-unexpected-performance" title="Permalink to this headline">¶</a></h3>
<p>This refers to any behaviour from the system that is not observed during the validation of the LEC and is not expected to be seen in the <em>normal operating conditions</em> of the system. A thing to note here is that the definition of <em>normal</em> is something that probably emerges from the system specifications during the design phase but it requires more refinement if we’re to correctly utilize this taxonomy.
This can happen because of unexpected feature interactions, and other emergent behaviour from the system that can cause problems when deployed in an environment where the conditions are different from the ones that the system was expected to run on.</p>
<div class="section" id="how-do-we-avoid-this-unexpected-performance">
<h4>How do we avoid this unexpected performance?<a class="headerlink" href="#how-do-we-avoid-this-unexpected-performance" title="Permalink to this headline">¶</a></h4>
<p>Compared to the case of expected performance, this requires runtime detection, recovery, and architecture-level safeguards to prevent hazardous states from emerging.</p>
</div>
<div class="section" id="unexpected-and-accurate-ua-hcm">
<h4>Unexpected and accurate (UA) HCM<a class="headerlink" href="#unexpected-and-accurate-ua-hcm" title="Permalink to this headline">¶</a></h4>
<p>This can occur in a situation where there is no notion of the ground truth.
An example given in the paper that illustrates the other condition under which UA HCM can emerge is when there rules are learned that might be completely appropriate under many circumstances but those that fail to capture established conventions that are known through implicit knowledge.</p>
<p>The example goes as follows: in uncontrolled airspace, if there are two planes that suddenly find themselves on a head-on collision course, the convention dictates that both of them bank to their respective rights to avoid collision.
But, if an automated aviation system is to learn the rules on its own, it might figure out that in many cases banking to the left saves it from collision with approaching objects and in this case would learn something that would clearly cause a collision when the human follows the convention and the automated system doesn’t know about that convention.</p>
<p><strong>What is covariate shift?</strong></p>
<p>It is a change in the dataset distribution, specifically on the inputs to a system that have a different distribution from the one that it was trained on.</p>
<p><strong>What is prior shift?</strong></p>
<p>Another kind of change in the dataset distribution when during operation the distribution of the outputs is different from the ones that it produced during training.</p>
<p><strong>What is concept shift?</strong></p>
<p>Combining both of the above in a sense, this is a shift in the joint distribution of inputs and outputs during operation compared to what the LEC had during training.</p>
</div>
</div>
<div class="section" id="so-how-do-we-manage-hcms">
<h3>So how do we manage HCMs?<a class="headerlink" href="#so-how-do-we-manage-hcms" title="Permalink to this headline">¶</a></h3>
<div class="section" id="under-accurate-and-hazardous-output">
<h4>Under accurate and hazardous output<a class="headerlink" href="#under-accurate-and-hazardous-output" title="Permalink to this headline">¶</a></h4>
<p>We can take the following actions to mitigate the effects under this condition:</p>
<ul class="simple">
<li><p>if you have information on potential precursors to hazardous conditions, reflect them in the training and validation data.</p></li>
<li><p>penalize these hazardous precursors appropriately during the training phase.</p></li>
<li><p>comprehensive testing of the model and verification-based coverage of the precursors as identified above.</p></li>
</ul>
</div>
<div class="section" id="under-hazardous-output-with-error">
<h4>Under hazardous output with error<a class="headerlink" href="#under-hazardous-output-with-error" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p>have valid safety thresholds associated with the outputs of the LEC.</p></li>
<li><p>reflect these thresholds in the training and validation data.</p></li>
<li><p>reflect these approximately in the cost or loss functions during training.</p></li>
<li><p>comprehensive model testing and verification that considers errors in relation to the overall performance of the system.</p></li>
</ul>
</div>
<div class="section" id="potential-approaches">
<h4>Potential approaches<a class="headerlink" href="#potential-approaches" title="Permalink to this headline">¶</a></h4>
<p><strong>Prevention and Recovery</strong></p>
<ul class="simple">
<li><p>comprehensive definition of the operating environment making it as rich as possible.</p></li>
<li><p>sufficiency of the training and validation data such that they are balanced, accurate, and complete.</p></li>
<li><p>contextually-relevant model validation techniques</p></li>
</ul>
<p><strong>Layered Mitigation</strong></p>
<p><em>The nice thing with a nod to this approach in the paper is that it borrows from the best practices in cybersecurity as well where we don’t rely on a single layer of controls to protect the system</em></p>
<ul class="simple">
<li><p>monitor your system during runtime to find instances where it violates the safety thresholds that you have put in place.</p></li>
<li><p>have ways to disengage the system in case something goes wrong, akin to a circuit breaker. <strong>I am currently working on a paper for this, <a class="reference external" href="mailto:abhishek&#37;&#52;&#48;montrealethics&#46;ai">shoot me an email</a> if you’d like to collaborate on that!</strong></p></li>
<li><p>have redundant functions deployed in the LEC so that there is a way to reach consensus in case there is a potential to violate the safety thresholds and then look for consensus to reinforce the above two mechanisms.</p></li>
</ul>
</div>
</div>
<div class="section" id="safety-assurance-arguments">
<h3>Safety Assurance Arguments<a class="headerlink" href="#safety-assurance-arguments" title="Permalink to this headline">¶</a></h3>
<p>The following figure does a good job of showcasing what a safety assurance argumentation can look like:</p>
<p><img alt="Safety Assurance Argumentation Pattern" src="_images/safety_arguments.png" /></p>
<p>Basically, this provides an assurance mechanism that the identified HCMs are acceptably mitigated in a demonstrable manner for stakeholders who need to be assured that the system will behave within acceptable bounds in operation. <em>I encourage you to refer to the paper for all the details on what each of the symbols mean and how to build such diagrams for your systems.</em></p>
</div>
<div class="section" id="conclusion">
<h3>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline">¶</a></h3>
<p>Ultimately, this paper provides a neat framework for thinking about the safety implications of a system in operation and provides not only a taxonomy for thinking about the potential hazards, but also provides a framework for demonstrating the acceptability of different mitigation techniques.</p>
<p>The gap filled from previous work is that it provides a holistic approach towards linking the assurances during the lifecycle of an LEC with the evidence using HCMs.</p>
</div>
<div class="section" id="what-does-this-mean-for-actionable-ai-ethics">
<h3>What does this mean for Actionable AI Ethics?<a class="headerlink" href="#what-does-this-mean-for-actionable-ai-ethics" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>This is a great way to build more robustness into machine learning systems, especially from the perspective of having demonstrable and verifiable claims about the system.</p></li>
<li><p>Given that the techniques mentioned here are system-agnostic, both from the perspective of the type of system within which the machine learning components are embedded and the type of machine learning itself, it serves as a great framework to complement other safety protocols within the organization.</p></li>
<li><p>The assurance argumentation graphs also help to unearth potential blindspots when trying to ensure the safety of such systems and are a great supplement to reports that might need to be filed for auditing or regulatory purposes.</p></li>
</ul>
</div>
<div class="section" id="questions-that-i-am-exploring">
<h3>Questions that I am exploring<a class="headerlink" href="#questions-that-i-am-exploring" title="Permalink to this headline">¶</a></h3>
<p><strong>If you have answers to any of these questions, please <a class="reference external" href="https://twitter.com/actionable_ai/status/1322406706073227264?s=20">tweet</a> and let me know!</strong></p>
<ol class="simple">
<li><p>To what extent are these being practised in the industry today?</p></li>
<li><p>What is the degree of awareness on the part of the practitioners and how rigorously do they adhere to these considerations?</p></li>
<li><p>I really liked the <strong>layered mitigation</strong> approach and wonder if there are exemplars of this in practice?</p></li>
</ol>
</div>
<div class="section" id="potential-further-reading">
<h3>Potential further reading<a class="headerlink" href="#potential-further-reading" title="Permalink to this headline">¶</a></h3>
<p>A list of papers that I think might be interesting related to this paper.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/1912.06166">ABOUT ML: Annotation and Benchmarking on Understanding and Transparency of Machine Learning Lifecycles</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1911.11932">Survey of Attacks and Defenses on Edge-Deployed Neural Networks</a></p></li>
<li><p><a class="reference external" href="https://ieeexplore.ieee.org/document/8812789">Disruptive Innovations and Disruptive Assurance: Assuring Machine Learning and Autonomy</a></p></li>
</ul>
<p><em>Please note that this is a wish list of sorts and I haven’t read through the papers listed here unless specified otherwise (if I have read them, there will be a link from the entry to the page for that.)</em></p>
</div>
<div class="section" id="twitter-discussion">
<h3>Twitter discussion<a class="headerlink" href="#twitter-discussion" title="Permalink to this headline">¶</a></h3>
<p>I’ll write back here with interesting points that surface from the Twitter discussion.</p>
<p><em>If you have comments and would like to discuss more, please leave a <a class="reference external" href="https://twitter.com/actionable_ai/status/1322406706073227264?s=20">tweet here</a>.</em></p>
</div>
<div class="section" id="sign-up-for-the-newsletter">
<h3>Sign up for the newsletter<a class="headerlink" href="#sign-up-for-the-newsletter" title="Permalink to this headline">¶</a></h3>
<iframe src="https://actionableaiethics.substack.com/embed" width="480" height="320" style="border:1px solid #EEE; background:white;" frameborder="0" scrolling="no"></iframe>
<p>To stay up-to-date with the latest content in terms of what I am reading and thinking about, please subscribe to the <a class="reference external" href="https://actionableaiethics.substack.com">Actionable AI Ethics newsletter</a></p>
</div>
<div class="section" id="support-me-with-a-coffee">
<h3>Support me with a coffee<a class="headerlink" href="#support-me-with-a-coffee" title="Permalink to this headline">¶</a></h3>
<p>If you enjoy the content on this page, you can support my work by <a class="reference external" href="https://buymeacoffee.com/abhishekgupta">buying me a coffee</a></p>
<script type="text/javascript" src="https://cdnjs.buymeacoffee.com/1.0.0/button.prod.min.js" data-name="bmc-button" data-slug="abhishekgupta" data-color="#FF5F5F" data-emoji=""  data-font="Cookie" data-text="Buy me a coffee" data-outline-color="#000000" data-font-color="#ffffff" data-coffee-color="#FFDD00" ></script></div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="automating_informality.html" title="previous page">Automating informality: On AI and labour in the global South</a>
    <a class='right-next' id="next-link" href="lets_talk_privacy.html" title="next page">Project Let’s Talk Privacy Full Report</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Abhishek Gupta<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="_static/js/index.js"></script>
    
  </body>
</html>